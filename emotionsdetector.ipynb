{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e62a4d-70c9-4f74-8953-f295b4b0a34b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dae8ebc8-d3cf-44dc-9db0-af49aada08b0",
   "metadata": {},
   "source": [
    "# User\n",
    "emotion detector using using laptop's camera emotions like happy, sad, angry, etc. along with percentage of it being that emotion should be displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9e5de7-cfd5-4114-94e3-f9bb8632af36",
   "metadata": {},
   "source": [
    "# Assistant\n",
    "# Emotion Detector Using Laptop's Camera\n",
    "\n",
    "I'll help you create an emotion detector that uses your laptop's camera to identify emotions like happy, sad, angry, etc., and displays the percentage confidence for each emotion.\n",
    "\n",
    "For this task, we'll need:\n",
    "1. Access to the laptop's camera\n",
    "2. A pre-trained emotion detection model\n",
    "3. Real-time processing capabilities\n",
    "\n",
    "Here's a complete solution using Python with the following libraries:\n",
    "- OpenCV (for camera access and image processing)\n",
    "- TensorFlow/Keras (for the emotion detection model)\n",
    "- NumPy (for numerical operations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a57751f2-9d05-4755-8a5a-f7bff17dd297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not load emotion model.\n",
      "Please download a pre-trained emotion recognition model or train your own.\n",
      "You can find pre-trained models at: https://github.com/oarriaga/face_classification\n",
      "Emotion detector started. Press 'q' to quit.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'emotion_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m roi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(roi, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m preds \u001b[38;5;241m=\u001b[39m emotion_model\u001b[38;5;241m.\u001b[39mpredict(roi)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Display emotion predictions with percentages\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (emotion, prob) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(emotion_labels, preds)):\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Format probability as percentage\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'emotion_model' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import time\n",
    "\n",
    "# Load pre-trained face detection model\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load pre-trained emotion detection model\n",
    "# Note: You'll need to download this model or train your own\n",
    "# For this example, we'll use a placeholder path - replace with your model path\n",
    "try:\n",
    "    emotion_model = load_model('emotion_model.h5')\n",
    "    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "except:\n",
    "    print(\"Error: Could not load emotion model.\")\n",
    "    print(\"Please download a pre-trained emotion recognition model or train your own.\")\n",
    "    print(\"You can find pre-trained models at: https://github.com/oarriaga/face_classification\")\n",
    "    exit()\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Emotion detector started. Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image\")\n",
    "        break\n",
    "        \n",
    "    # Create a copy of the frame to draw on\n",
    "    canvas = frame.copy()\n",
    "    \n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    # For each face detected\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw rectangle around face\n",
    "        cv2.rectangle(canvas, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Extract face ROI (Region of Interest)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        \n",
    "        # Resize to model input size (typically 48x48 for emotion models)\n",
    "        roi = cv2.resize(roi_gray, (48, 48))\n",
    "        roi = roi.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "        \n",
    "        # Make prediction\n",
    "        preds = emotion_model.predict(roi)[0]\n",
    "        \n",
    "        # Display emotion predictions with percentages\n",
    "        for i, (emotion, prob) in enumerate(zip(emotion_labels, preds)):\n",
    "            # Format probability as percentage\n",
    "            percentage = prob * 100\n",
    "            \n",
    "            # Display text\n",
    "            text = f\"{emotion}: {percentage:.2f}%\"\n",
    "            y_pos = y - 10 + i * 20  # Position text above face, with each emotion on a new line\n",
    "            cv2.putText(canvas, text, (x, y_pos), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Highlight the emotion with highest probability\n",
    "        emotion_idx = np.argmax(preds)\n",
    "        label = f\"{emotion_labels[emotion_idx]}: {preds[emotion_idx]*100:.2f}%\"\n",
    "        cv2.putText(canvas, label, (x, y+h+20), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Emotion Detector', canvas)\n",
    "    \n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56fa5b0-3299-4e94-944d-80885d499cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10bfe333-a56c-47c4-8379-d450990c459c",
   "metadata": {},
   "source": [
    "\n",
    "## Important Notes:\n",
    "\n",
    "1. **Model Requirement**: You'll need a pre-trained emotion detection model. Popular options include:\n",
    "   - FER2013 dataset-based models\n",
    "   - DeepFace library's emotion models\n",
    "   - Models from the face_classification repository\n",
    "\n",
    "2. **Installation Requirements**:\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45949592-415c-40ab-ab49-a6319efa2c10",
   "metadata": {},
   "source": [
    "   pip install opencv-python tensorflow numpy\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca6d9c-1d38-4344-919c-e68a9c4679db",
   "metadata": {},
   "source": [
    "\n",
    "3. **Alternative Approach**: If you don't want to handle the model yourself, you could use the DeepFace library which has emotion detection built-in:\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d5c1a7-459f-476a-b66d-6f2045ce65e3",
   "metadata": {},
   "source": [
    "   pip install deepface\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bfb3ac-98f0-4f1d-a815-9d136b91ddba",
   "metadata": {},
   "source": [
    "\n",
    "4. **Performance**: Emotion detection accuracy varies based on lighting conditions, face angles, and the quality of the model used.\n",
    "\n",
    "5. **Privacy Considerations**: Since this application uses the camera, be mindful of privacy implications when using or distributing this code.\n",
    "\n",
    "To improve accuracy, consider adjusting the face detection parameters or using a more sophisticated face detection model like MTCNN or RetinaFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daec546-4f36-4f52-98cf-9e416d69c417",
   "metadata": {},
   "source": [
    "# User\n",
    "emotion detector using using laptop's camera emotions like happy, sad, angry, etc. along with percentage of it being that emotion should be displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e8bd3c-722a-490e-8fca-42b6f5363cc9",
   "metadata": {},
   "source": [
    "# Assistant\n",
    "# Emotion Detector Using Laptop's Camera\n",
    "\n",
    "I'll help you create an emotion detector that uses your laptop's camera to identify emotions like happy, sad, angry, etc., and displays the percentage confidence for each emotion.\n",
    "\n",
    "For this task, we'll need:\n",
    "1. Access to the laptop's camera\n",
    "2. A pre-trained emotion detection model\n",
    "3. Real-time processing capabilities\n",
    "\n",
    "Here's a complete solution using Python with the following libraries:\n",
    "- OpenCV (for camera access and image processing)\n",
    "- TensorFlow/Keras (for the emotion detection model)\n",
    "- NumPy (for numerical operations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab01873-cd94-435b-8c91-869fc4eea962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import time\n",
    "\n",
    "# Load pre-trained face detection model\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load pre-trained emotion detection model\n",
    "# Note: You'll need to download this model or train your own\n",
    "# For this example, we'll use a placeholder path - replace with your model path\n",
    "try:\n",
    "    emotion_model = load_model('emotion_model.h5')\n",
    "    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "except:\n",
    "    print(\"Error: Could not load emotion model.\")\n",
    "    print(\"Please download a pre-trained emotion recognition model or train your own.\")\n",
    "    print(\"You can find pre-trained models at: https://github.com/oarriaga/face_classification\")\n",
    "    exit()\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Emotion detector started. Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image\")\n",
    "        break\n",
    "        \n",
    "    # Create a copy of the frame to draw on\n",
    "    canvas = frame.copy()\n",
    "    \n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    # For each face detected\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw rectangle around face\n",
    "        cv2.rectangle(canvas, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Extract face ROI (Region of Interest)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        \n",
    "        # Resize to model input size (typically 48x48 for emotion models)\n",
    "        roi = cv2.resize(roi_gray, (48, 48))\n",
    "        roi = roi.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "        \n",
    "        # Make prediction\n",
    "        preds = emotion_model.predict(roi)[0]\n",
    "        \n",
    "        # Display emotion predictions with percentages\n",
    "        for i, (emotion, prob) in enumerate(zip(emotion_labels, preds)):\n",
    "            # Format probability as percentage\n",
    "            percentage = prob * 100\n",
    "            \n",
    "            # Display text\n",
    "            text = f\"{emotion}: {percentage:.2f}%\"\n",
    "            y_pos = y - 10 + i * 20  # Position text above face, with each emotion on a new line\n",
    "            cv2.putText(canvas, text, (x, y_pos), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Highlight the emotion with highest probability\n",
    "        emotion_idx = np.argmax(preds)\n",
    "        label = f\"{emotion_labels[emotion_idx]}: {preds[emotion_idx]*100:.2f}%\"\n",
    "        cv2.putText(canvas, label, (x, y+h+20), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Emotion Detector', canvas)\n",
    "    \n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c65b2-6292-46ec-8bea-e57988ca4f6a",
   "metadata": {},
   "source": [
    "\n",
    "## Important Notes:\n",
    "\n",
    "1. **Model Requirement**: You'll need a pre-trained emotion detection model. Popular options include:\n",
    "   - FER2013 dataset-based models\n",
    "   - DeepFace library's emotion models\n",
    "   - Models from the face_classification repository\n",
    "\n",
    "2. **Installation Requirements**:\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237d3bf7-ba3f-4528-a36e-1b8d76a4a535",
   "metadata": {},
   "source": [
    "   pip install opencv-python tensorflow numpy\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655049c9-9ded-4b3a-b237-a185105d8a77",
   "metadata": {},
   "source": [
    "\n",
    "3. **Alternative Approach**: If you don't want to handle the model yourself, you could use the DeepFace library which has emotion detection built-in:\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe1be6b-6f3e-4bcf-a3e3-193f6b09a7c6",
   "metadata": {},
   "source": [
    "   pip install deepface\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f1bda-07f8-46a0-b6a6-9e840ec86a95",
   "metadata": {},
   "source": [
    "\n",
    "4. **Performance**: Emotion detection accuracy varies based on lighting conditions, face angles, and the quality of the model used.\n",
    "\n",
    "5. **Privacy Considerations**: Since this application uses the camera, be mindful of privacy implications when using or distributing this code.\n",
    "\n",
    "To improve accuracy, consider adjusting the face detection parameters or using a more sophisticated face detection model like MTCNN or RetinaFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee386a92-1948-455a-81fb-8f64f12a2ac7",
   "metadata": {},
   "source": [
    "# User\n",
    "use deepface instead of pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198e1bc3-52c7-46f9-bda8-047fd127b640",
   "metadata": {},
   "source": [
    "# Assistant\n",
    "# Emotion Detector Using DeepFace and Laptop's Camera\n",
    "\n",
    "I'll create an emotion detector using the DeepFace library, which provides pre-built emotion recognition capabilities. This approach is simpler as DeepFace handles the model loading and prediction for us.\n",
    "\n",
    "Here's a complete solution using your laptop's camera to detect emotions and display percentages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e124fe-295f-42b0-9d81-533963ec870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from deepface import DeepFace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "# Define emotion list for consistent ordering in display\n",
    "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "\n",
    "print(\"Emotion detector started. Press 'q' to quit.\")\n",
    "print(\"Note: First analysis may take a moment to load the models.\")\n",
    "\n",
    "# For controlling analysis frequency\n",
    "last_analysis_time = 0\n",
    "analysis_interval = 1  # Analyze every 1 second to reduce CPU usage\n",
    "\n",
    "# Store the last valid emotion result\n",
    "last_emotion_result = None\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image\")\n",
    "        break\n",
    "    \n",
    "    # Create a copy of the frame to draw on\n",
    "    canvas = frame.copy()\n",
    "    \n",
    "    current_time = time.time()\n",
    "    \n",
    "    # Only run analysis periodically to improve performance\n",
    "    if current_time - last_analysis_time >= analysis_interval:\n",
    "        try:\n",
    "            # Analyze emotions using DeepFace\n",
    "            result = DeepFace.analyze(frame, \n",
    "                                      actions=['emotion'], \n",
    "                                      enforce_detection=False,\n",
    "                                      silent=True)\n",
    "            \n",
    "            if result:\n",
    "                last_emotion_result = result[0] if isinstance(result, list) else result\n",
    "                last_analysis_time = current_time\n",
    "        except Exception as e:\n",
    "            print(f\"Analysis error: {e}\")\n",
    "    \n",
    "    # If we have emotion results, display them\n",
    "    if last_emotion_result is not None:\n",
    "        # Get face coordinates if available\n",
    "        if 'region' in last_emotion_result:\n",
    "            x = last_emotion_result['region']['x']\n",
    "            y = last_emotion_result['region']['y']\n",
    "            w = last_emotion_result['region']['w']\n",
    "            h = last_emotion_result['region']['h']\n",
    "            \n",
    "            # Draw rectangle around face\n",
    "            cv2.rectangle(canvas, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Get emotion data\n",
    "        emotions = last_emotion_result['emotion']\n",
    "        \n",
    "        # Display all emotions with percentages\n",
    "        y_offset = 30\n",
    "        for i, emotion in enumerate(emotion_labels):\n",
    "            if emotion in emotions:\n",
    "                percentage = emotions[emotion]\n",
    "                text = f\"{emotion}: {percentage:.2f}%\"\n",
    "                cv2.putText(canvas, text, (10, y_offset + i*30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Find dominant emotion\n",
    "        dominant_emotion = last_emotion_result['dominant_emotion']\n",
    "        dominant_score = emotions[dominant_emotion]\n",
    "        \n",
    "        # Display dominant emotion\n",
    "        cv2.putText(canvas, f\"Dominant: {dominant_emotion} ({dominant_score:.2f}%)\", \n",
    "                    (10, y_offset + len(emotion_labels)*30 + 20), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "    \n",
    "    # Display status message when analyzing\n",
    "    if current_time - last_analysis_time < 0.3:  # Show \"Analyzing\" briefly\n",
    "        cv2.putText(canvas, \"Analyzing...\", (canvas.shape[1] - 150, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Emotion Detector (DeepFace)', canvas)\n",
    "    \n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5614526f-7119-47aa-b1d0-5e72c5a9fa5a",
   "metadata": {},
   "source": [
    "# User\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from deepface import DeepFace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "# Define emotion list for consistent ordering in display\n",
    "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "\n",
    "print(\"Emotion detector started. Press 'q' to quit.\")\n",
    "print(\"Note: First analysis may take a moment to load the models.\")\n",
    "\n",
    "# For controlling analysis frequency\n",
    "last_analysis_time = 0\n",
    "analysis_interval = 1  # Analyze every 1 second to reduce CPU usage\n",
    "\n",
    "# Store the last valid emotion result\n",
    "last_emotion_result = None\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image\")\n",
    "        break\n",
    "    \n",
    "    # Create a copy of the frame to draw on\n",
    "    canvas = frame.copy()\n",
    "    \n",
    "    current_time = time.time()\n",
    "    \n",
    "    # Only run analysis periodically to improve performance\n",
    "    if current_time - last_analysis_time >= analysis_interval:\n",
    "        try:\n",
    "            # Analyze emotions using DeepFace\n",
    "            result = DeepFace.analyze(frame, \n",
    "                                      actions=['emotion'], \n",
    "                                      enforce_detection=False,\n",
    "                                      silent=True)\n",
    "            \n",
    "            if result:\n",
    "                last_emotion_result = result[0] if isinstance(result, list) else result\n",
    "                last_analysis_time = current_time\n",
    "        except Exception as e:\n",
    "            print(f\"Analysis error: {e}\")\n",
    "    \n",
    "    # If we have emotion results, display them\n",
    "    if last_emotion_result is not None:\n",
    "        # Get face coordinates if available\n",
    "        if 'region' in last_emotion_result:\n",
    "            x = last_emotion_result['region']['x']\n",
    "            y = last_emotion_result['region']['y']\n",
    "            w = last_emotion_result['region']['w']\n",
    "            h = last_emotion_result['region']['h']\n",
    "            \n",
    "            # Draw rectangle around face\n",
    "            cv2.rectangle(canvas, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Get emotion data\n",
    "        emotions = last_emotion_result['emotion']\n",
    "        \n",
    "        # Display all emotions with percentages\n",
    "        y_offset = 30\n",
    "        for i, emotion in enumerate(emotion_labels):\n",
    "            if emotion in emotions:\n",
    "                percentage = emotions[emotion]\n",
    "                text = f\"{emotion}: {percentage:.2f}%\"\n",
    "                cv2.putText(canvas, text, (10, y_offset + i*30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Find dominant emotion\n",
    "        dominant_emotion = last_emotion_result['dominant_emotion']\n",
    "        dominant_score = emotions[dominant_emotion]\n",
    "        \n",
    "        # Display dominant emotion\n",
    "        cv2.putText(canvas, f\"Dominant: {dominant_emotion} ({dominant_score:.2f}%)\", \n",
    "                    (10, y_offset + len(emotion_labels)*30 + 20), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "    \n",
    "    # Display status message when analyzing\n",
    "    if current_time - last_analysis_time < 0.3:  # Show \"Analyzing\" briefly\n",
    "        cv2.putText(canvas, \"Analyzing...\", (canvas.shape[1] - 150, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Emotion Detector (DeepFace)', canvas)\n",
    "    \n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "not loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c54248d-d647-4dbe-abaf-427d267ffaf4",
   "metadata": {},
   "source": [
    "# Assistant\n",
    "# Troubleshooting DeepFace Emotion Detection in Jupyter Notebook\n",
    "\n",
    "It looks like you're experiencing an issue with your emotion detection code not loading properly. Let's troubleshoot this step by step.\n",
    "\n",
    "## Common Issues and Solutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "421d6118-d63c-404e-8fdc-374d5edeac79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Nikhil\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Testing camera access...\n",
      "Camera test: SUCCESS\n",
      "\n",
      "Testing DeepFace (this may take a moment to download models)...\n",
      "Analyzing test image...\n",
      "DeepFace test: SUCCESS\n",
      "Detected emotion: neutral\n",
      "\n",
      "Diagnostic complete. Your system appears ready to run the emotion detector.\n",
      "If you're still having issues, try the following:\n",
      "1. Restart your Jupyter kernel\n",
      "2. Ensure you have proper lighting for face detection\n",
      "3. Try reducing the resolution if performance is slow\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "# First, let's check if the camera is accessible\n",
    "def test_camera_and_deepface():\n",
    "    # Test camera\n",
    "    print(\"Testing camera access...\")\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"ERROR: Cannot access webcam. Check connections or permissions.\")\n",
    "        return False\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"ERROR: Cannot read frame from webcam.\")\n",
    "        cap.release()\n",
    "        return False\n",
    "    \n",
    "    print(\"Camera test: SUCCESS\")\n",
    "    \n",
    "    # Test DeepFace\n",
    "    print(\"\\nTesting DeepFace (this may take a moment to download models)...\")\n",
    "    try:\n",
    "        # Save frame temporarily\n",
    "        test_img_path = \"test_frame.jpg\"\n",
    "        cv2.imwrite(test_img_path, frame)\n",
    "        \n",
    "        # Try analyzing a static image first\n",
    "        print(\"Analyzing test image...\")\n",
    "        result = DeepFace.analyze(img_path=test_img_path, \n",
    "                                 actions=['emotion'],\n",
    "                                 enforce_detection=False)\n",
    "        print(\"DeepFace test: SUCCESS\")\n",
    "        print(f\"Detected emotion: {result[0]['dominant_emotion'] if isinstance(result, list) else result['dominant_emotion']}\")\n",
    "        \n",
    "        # Clean up\n",
    "        if os.path.exists(test_img_path):\n",
    "            os.remove(test_img_path)\n",
    "        \n",
    "        cap.release()\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR with DeepFace: {str(e)}\")\n",
    "        if os.path.exists(test_img_path):\n",
    "            os.remove(test_img_path)\n",
    "        cap.release()\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "test_result = test_camera_and_deepface()\n",
    "\n",
    "if test_result:\n",
    "    print(\"\\nDiagnostic complete. Your system appears ready to run the emotion detector.\")\n",
    "    print(\"If you're still having issues, try the following:\")\n",
    "    print(\"1. Restart your Jupyter kernel\")\n",
    "    print(\"2. Ensure you have proper lighting for face detection\")\n",
    "    print(\"3. Try reducing the resolution if performance is slow\")\n",
    "else:\n",
    "    print(\"\\nDiagnostic failed. Please check the errors above.\")\n",
    "    print(\"Common solutions:\")\n",
    "    print(\"1. Install missing dependencies: pip install deepface opencv-python\")\n",
    "    print(\"2. Check webcam permissions for your browser/system\")\n",
    "    print(\"3. Try a different webcam if available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba00495-b265-410d-b219-193b91c80576",
   "metadata": {},
   "source": [
    "\n",
    "## Additional Troubleshooting Tips\n",
    "\n",
    "If the diagnostic code above doesn't resolve your issue, here are some additional steps to try:\n",
    "\n",
    "1. **Model Download Issues**: DeepFace downloads models on first use. Ensure you have internet connectivity and sufficient disk space.\n",
    "\n",
    "2. **Jupyter-Specific Issues**:\n",
    "   - Some browsers may block webcam access in Jupyter. Try using Chrome or Firefox.\n",
    "   - Run `jupyter notebook --NotebookApp.iopub_data_rate_limit=1e10` to increase data rate limits.\n",
    "\n",
    "3. **Package Versions**:\n",
    "   - DeepFace requires specific versions of TensorFlow and other dependencies.\n",
    "   - Try creating a fresh environment: `conda create -n emotion_detection python=3.8`\n",
    "   - Install packages: `pip install deepface opencv-python matplotlib numpy`\n",
    "\n",
    "4. **Alternative Approach**:\n",
    "   - If webcam access is problematic in Jupyter, consider running your script as a standalone Python file.\n",
    "   - Save your code as `emotion_detector.py` and run it from the command line.\n",
    "\n",
    "5. **Performance Optimization**:\n",
    "   - Lower the resolution: `cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)` and `cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)`\n",
    "   - Increase the analysis interval to reduce CPU usage\n",
    "\n",
    "Remember that the first run of DeepFace will be slow as it downloads the required models. Subsequent runs should be faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de3883-6ace-445a-90d2-c53a04862b75",
   "metadata": {},
   "source": [
    "\n",
    "## Important Notes:\n",
    "\n",
    "1. **Installation Requirements**:\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed199048-e9ac-4825-83b3-240a9c85cf49",
   "metadata": {},
   "source": [
    "   pip install deepface opencv-python numpy matplotlib\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0db826-930b-4abb-9bcb-81227e7ba5dd",
   "metadata": {},
   "source": [
    "\n",
    "2. **First-Run Performance**: The first time you run DeepFace, it will download the necessary models, which might take a moment. Subsequent runs will be faster.\n",
    "\n",
    "3. **Analysis Frequency**: The code analyzes emotions once per second to reduce CPU usage. You can adjust the `analysis_interval` variable to change this.\n",
    "\n",
    "4. **Face Detection**: DeepFace will attempt to detect faces automatically. The `enforce_detection=False` parameter allows the analysis to continue even if a face isn't clearly detected.\n",
    "\n",
    "5. **Advantages of DeepFace**:\n",
    "   - No need to manually download or manage models\n",
    "   - Includes multiple pre-trained models for emotion recognition\n",
    "   - Provides additional capabilities like age/gender detection if needed\n",
    "\n",
    "6. **Performance Considerations**:\n",
    "   - DeepFace is more resource-intensive than lightweight custom models\n",
    "   - The analysis is throttled to improve performance\n",
    "   - For better performance on slower machines, you could reduce the frame resolution\n",
    "\n",
    "7. **Privacy**: As with any camera application, be mindful of privacy considerations when using this code.\n",
    "\n",
    "This implementation provides a clean, easy-to-use emotion detector that displays percentages for each emotion and highlights the dominant one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8180df44-3f38-4781-915d-ad2f9fb7bc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e21a88-e1d5-4e7c-b3eb-398a525e1753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00073e37-74c9-42ae-9911-184b2db01a68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5129e-e21b-49ea-a0b4-3bd1d144565e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
